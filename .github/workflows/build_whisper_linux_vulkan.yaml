name: Compile Whisper model

on:
  workflow_dispatch:
    inputs:
      quantization:
        description: "Choose quantization for model"
        required: true
        default: "q0f32"
        type: choice
        options:
          - q0f32
          - q4f32_1
   

jobs:

  compile-wisper-model:
    runs-on: ubuntu-latest
    container:
      image: ghcr.io/olyasir/models_build:ubuntu22.04_vulkan

    steps:
      
      - name: Check out repository
        uses: actions/checkout@v3
        with:
          ref: base/v1
          fetch-depth: 1  # Only fetch the latest commit

      - name: Install Rust manually
        run: |
          curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH  # Add Rust to the PATH for GitHub Actions

      - name: Source Rust environment
        shell: bash
        run: source $HOME/.cargo/env && rustup --version && cargo --version

      - name: Mark repository as safe
        run: |
          git config --global --add safe.directory /__w/mlc-llm/mlc-llm

      - name: get submodules
        run: |
            git submodule update --init --recursive

      - name: install tvm
        run: |
            cd /tvm/python
            pip install -e .

      - name: verify tvm installation
        run: |
            echo "Verifying tvm instalation "
            python -c "import tvm; print(tvm.__file__)"


      - name: Run a command in the container
        run: |
          echo "Verifying vulkan instalation "
          python --version
          vulkaninfo
          cargo --version
          pwd 
          rustup --version

    
      - name: Download model
        run: git-lfs install && git clone https://huggingface.co/openai/whisper-tiny

      - name: Create build directory
        shell: bash
        run: |
            mkdir build && cd build && cp ../cmake/config.cmake .
            echo "set(USE_VULKAN ON)" >> config.cmake
            echo "set(TVM_SOURCE_DIR /tvm)" >> config.cmake
            cmake ..

      - name: Build the project
        shell: bash
        run: |
           source $HOME/.cargo/env && cd build && cmake --build . 

      - name: cleanup #needed as there is only 15G on runner, not enough for mlc-llm instalation
        run: rm -rf $HOME/.cargo $HOME/.rustup 3rdpary/tvm  /__t/CodeQL

      - name: Install mlc-llm
        run: |
           cd python && pip install -e .

      - name: Compile model
        run: |
            ./ci/models_build/whisper.sh "${{ github.event.inputs.quantization }}" vulkan
      - name: fix mlc-chat-config.json
        run: |
            apt-get install jq
            jq '.forced_decoder_ids = [[1,50259],[2,50359],[3,50364]]' output-whisper-${{ github.event.inputs.quantization }}/mlc-chat-config.json > temp.json && mv temp.json output-whisper-${{ github.event.inputs.quantization }}/mlc-chat-config.json
      - name: Upload compied model as artifact
        uses: actions/upload-artifact@v3
        with:
          name: output-whisper-${{ github.event.inputs.quantization }}
          path: output-whisper-${{ github.event.inputs.quantization }}/
            


